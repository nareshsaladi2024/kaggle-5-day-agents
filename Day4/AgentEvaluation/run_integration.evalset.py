with open("home_automation_agent/integration.evalset.json", "w") as f:
    json.dump(test_cases, f, indent=2)

print("âœ… Evaluation test cases created")
print("\nğŸ§ª Test scenarios:")
for case in test_cases["eval_cases"]:
    user_msg = case["conversation"][0]["user_content"]["parts"][0]["text"]
    print(f"â€¢ {case['eval_id']}: {user_msg}")

print("\nğŸ“Š Expected results:")
print("â€¢ basic_device_control: Should pass both criteria")
print(
    "â€¢ wrong_tool_usage_test: May fail tool_trajectory if agent uses wrong parameters"
)
print(
    "â€¢ poor_response_quality_test: May fail response_match if response differs too much"
)
# Analyzing evaluation results - the data science approach
print("ğŸ“Š Understanding Evaluation Results:")
print()
print("ğŸ” EXAMPLE ANALYSIS:")
print()
print("Test Case: living_room_light_on")
print("  âŒ response_match_score: 0.45/0.80")
print("  âœ… tool_trajectory_avg_score: 1.0/1.0")
print()
print("ğŸ“ˆ What this tells us:")
print("â€¢ TOOL USAGE: Perfect - Agent used correct tool with correct parameters")
print("â€¢ RESPONSE QUALITY: Poor - Response text too different from expected")
print("â€¢ ROOT CAUSE: Agent's communication style, not functionality")
print()
print("ğŸ¯ ACTIONABLE INSIGHTS:")
print("1. Technical capability works (tool usage perfect)")
print("2. Communication needs improvement (response quality failed)")
print("3. Fix: Update agent instructions for clearer language or constrained response.")
print()